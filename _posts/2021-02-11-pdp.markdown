---
title:  "[Interpretable ML] #1. Partial Dependence Plot(PDP)"
categories:
  - theory
tags:
  - interpretable machine learning
  - PDP
  - global interpretation
toc: true
use_math: true
sitemap: 
---

최근에 마친 포스팅 시리즈인 Feature Selection 파트를 준비하면서 자연스럽게 머신러닝의 해석에 관한 이론들도 접하게 되었다. 머신러닝 모델링을 할 때 중요한 feature만을 선택해 모델의 성능을 높이는 것도 중요하지만, 더 나아가 완성된 모델을 어떻게 해석할 것인지가 관건이다. 예를 들어 분류 모델에서 단순히 데이터들을 0, 1, 2로 분류하는 것에서 끝나는 것이 아니라, 어떤 변수가 데이터가 1로 분류되는데에 어떤 영향을 미쳤는지, 변수간의 interaction은 어떻게 되는지 등등을 해석하는 것이 중요하다. (머신러닝 모델에 대한 해석은 비단 모델링 성능을 높이는 데에만 기여하는 것이 아니라 다른 insight를 얻는 데에도 기여할 수 있다.)

머신러닝의 해석 첫 포스팅에서는 머신러닝 해석에 가장 기본적으로 사용되는 plot인 <mark style='background-color: #fff5b1'> Partial Dependence Plot </mark>에 대해 다루도록 하겠다.

## Partial Dependence Plot
PDP는 특정 feature가 target에 미치는 한계효과(marginal effect)를 파악하고자 할 때 사용하는 plot이다. 간단히 수식으로 먼저 살펴보자!
Partial dependence function은 다음과 같이 정의된다.
$ \hat{f_{x_{S}}} = \int \hat_{f(x_{S}, x_{C})} \dP(x_{C}) = \frac {1}{n} \sum_{i=1}^{n} \hat_{f(x_{S}, x_{C}^{(i)})} $
여기서 $ x_{C}$는 PDP에서 해석하고자하는 feature set을 나타내고 나머지 해석에 관심이 없는 변수들의 set은 $x_{C}$로 나타낸다. Partial dependence function은 $x_{S}$ 의 한계효과를 계산하는 것이 목적이기 때문에, $x_{S}$에 대해서 integral 혹은 sigma sum을 적용함으로써 얻을 수 있다. 이 때 주의해야 할 점은 partial dependence function을 구할 때 $x_{S}$와 $x_{C}$는 독립이어야 한다. 
