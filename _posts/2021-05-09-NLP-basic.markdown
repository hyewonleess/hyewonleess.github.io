---
title:  "[NLP] #1. 워드 임베딩(Word Embedding)"
categories:
  - deeplearning
tags:
  - NLP
  - deeplearning
  - DL
  - text
  - tensorflow
toc: true
use_math: true
sitemap: 
---

# Prologue
**자연어처리(Natural Language Preprocessing)** 은 문장, 텍스트 등 인간의 언어를 컴퓨터 언어로 구현하는 AI 알고리즘이다. NLP는 현 시대를 대표는 AI 알고리즘 중 하나로, 가장 대표적인 것이 텍스트분석인데, 예를 들면 영화 감상평 분석, 
트위터를 이용한 의견 분석 등등을 할 때 쓰이는 분석기법이다. 

![pic](/assets/nlp.PNG)

(이미지 출처: <https://brunch.co.kr/@natrsci/83>)

<br>
대학원 딥러닝 수업에서 처음으로 자연어처리에 대해 배웠는데, 공부할 것도 많고 알아가야 하는 것이 많아서 포스팅으로 남기려고 한다. 이 블로그에서 다루는 자연어처리 포스팅은 대부분 텍스트분석에 관련된 내용일 것 같다 :) 
<br>

# Word Embedding(워드 임베딩)
워드임베딩은 텍스트를 구성하는 단어를 벡터로 표현하는 것이다. 쉽게 말하면, 단어를 밀집벡터(Dense vector)로 표현해 컴퓨터가 처리할 수 있게 변환하는 과정을 말한다. 본격적인 워드임베딩 방법론에 대해 다루기 전에 알아야 하는 용어들에 대해 살펴보자.
<br>


## 1. 희소벡터 vs 밀집벡터
### 희소벡터(Sparse Vector)
데이터분석에 대해 혹은 선형대수학을 공부했다면 희소행렬에 대해 들어보았을 것이다! 희소벡터(Sparse vector)도 비슷한 개념이다. 

예를 들면 텍스트에서 "That dog is cute" 라는 문장이 있다고 하면, 이 문장은 the, dog, is, cute 의 네 단어로 이루어져 있다. 희소벡터는 각 단어가 있으면 1, 없으면 0으로 표현하는 것이다. 
각 단어는 지정된 위치가 있다. 전체 텍스트에서 unique한 단어의 수가 8개라고 할 때 "The dog is cute"를 희소벡터로 나타내면 [0, 1, 0, 0, 1, 1, 0, 0, 1] 와 같다.

희소벡터는 곧 One-hot vector와 동일한 것이다. 데이터분석을 할 때 one-hot vector를 카테고리형 자료를 처리하는데 유용하다고 하지만 텍스트분석 시 이러한 표현 방법은 큰 단점이 있다.
 1. 단어의 수가 많아지면 벡터의 dimension이 매우 커져 분석에 어려움이 생긴다. (계산복잡도 증가, 공간 낭비)
 2. 전체 텍스트에서 한 번 등장한 단어도 벡터에 포함되기 때문에 텍스트 처리 시 비효율적이다.


### 밀집벡터(Dense vector)
밀집벡터는 희소벡터의 dimension 문제를 보완한 벡터이다. 밀집벡터는 말 그대로 희소벡터를 압축한 것으로, 희소벡터가 전체 텍스트에 포함된 단어의 수를 벡터의 크기로 정하는데 반해 밀집벡터는 사용자가 설정한 벡터의 크기로 단어를 맞춘다.

"The dog is cute"를 희소벡터로 나타내면 [0, 1, 0, 0, 1, 1, 0, 0, 1] 이지만, 밀집벡터로 나타내면 [0.2, -0.1, 0.03, 0.5] 와 같다.


## 2. Word2vec
