---
title:  "[NLP] #1. 워드 임베딩(Word Embedding)"
categories:
  - deeplearning
tags:
  - NLP
  - deeplearning
  - DL
  - text
  - tensorflow
toc: true
use_math: true
sitemap: 
---

# Prologue
**자연어처리(Natural Language Preprocessing)** 은 문장, 텍스트 등 인간의 언어를 컴퓨터 언어로 구현하는 AI 알고리즘이다. NLP는 현 시대를 대표는 AI 알고리즘 중 하나로, 가장 대표적인 것이 텍스트분석인데, 예를 들면 영화 감상평 분석, 
트위터를 이용한 의견 분석 등등을 할 때 쓰이는 분석기법이다. 

![pic](/assets/nlp.PNG)

(이미지 출처: <https://brunch.co.kr/@natrsci/83>)

<br>
대학원 딥러닝 수업에서 처음으로 자연어처리에 대해 배웠는데, 공부할 것도 많고 알아가야 하는 것이 많아서 포스팅으로 남기려고 한다. 이 블로그에서 다루는 자연어처리 포스팅은 대부분 텍스트분석에 관련된 내용일 것 같다 :) 
<br>

# Word Embedding(워드 임베딩)
워드임베딩은 텍스트를 구성하는 단어를 벡터로 표현하는 것이다. 쉽게 말하면, 단어를 밀집벡터(Dense vector)로 표현해 컴퓨터가 처리할 수 있게 변환하는 과정을 말한다. 본격적인 워드임베딩 방법론에 대해 다루기 전에 알아야 하는 용어들에 대해 살펴보자.
<br>


## 1. 희소벡터 vs 밀집벡터
### 희소벡터(Sparse Vector)
데이터분석에 대해 혹은 선형대수학을 공부했다면 희소행렬에 대해 들어보았을 것이다! 희소벡터(Sparse vector)도 비슷한 개념이다. 

예를 들면 텍스트에서 "That dog is cute" 라는 문장이 있다고 하면, 이 문장은 the, dog, is, cute 의 네 단어로 이루어져 있다. 희소벡터는 각 단어가 있으면 1, 없으면 0으로 표현하는 것이다. 
각 단어는 지정된 위치가 있다. 전체 텍스트에서 unique한 단어의 수가 8개라고 할 때 "The dog is cute"를 희소벡터로 나타내면 [0, 1, 0, 0, 1, 1, 0, 0, 1] 와 같다.

희소벡터는 곧 One-hot vector와 동일한 것이다. 데이터분석을 할 때 one-hot vector를 카테고리형 자료를 처리하는데 유용하다고 하지만 텍스트분석 시 이러한 표현 방법은 큰 단점이 있다.
 1. 단어의 수가 많아지면 벡터의 dimension이 매우 커져 분석에 어려움이 생긴다. (계산복잡도 증가, 공간 낭비)
 2. 전체 텍스트에서 한 번 등장한 단어도 벡터에 포함되기 때문에 텍스트 처리 시 비효율적이다.


### 밀집벡터(Dense vector)
밀집벡터는 희소벡터의 dimension 문제를 보완한 벡터이다. 밀집벡터는 말 그대로 희소벡터를 압축한 것으로, 희소벡터가 전체 텍스트에 포함된 단어의 수를 벡터의 크기로 정하는데 반해 밀집벡터는 사용자가 설정한 벡터의 크기로 단어를 맞춘다.

"The dog is cute"를 희소벡터로 나타내면 [0, 1, 0, 0, 1, 1, 0, 0, 1] 이지만, 밀집벡터로 나타내면 [0.2, -0.1, 0.03, 0.5] 와 같다.


## 2. Word2vec
Word2vec은 워드임베딩 방법 중 하나이다. **워드임베딩**은 텍스트를 밀집벡터(Dense vector)의 형태로 변환하는 과정을 말한다. Word2vec에는 크게 **CBOW**와 **Skip-Gram** 이 있다. 이 두 방법은 Input과 output이 서로 반대일 뿐, 나머지 과정은 모두 동일하다.

### (1) CBOW
CBOW는 주변단어를 이용해 중심단어를 예측하는 방법이다. 

예시: A fat dog ate delicious meat
<br>
여기서 예측하고자 하는 중심단어가 **ate**라고 하자. CBOW에서는 window 라는 것을 결정하는데, 이는 중심단어 앞/뒤로 몇 개의 단어를 주변단어로 포함시킬지를 결정하는 요소이다. 만약 window가 2라면 ate 앞의 두 단어 fat, dog와 뒤의 두 단어 delicious, mean이 중심단어이다. 즉, $2 \times \text{window}$ 만큼 중심단어의 개수를 정한다.

<br>
아래 그림은 CBOW의 매커니즘을 도식화한 것이다.

 + **Input layer**: 주변단어 4개($1 \times v$ vector)를 input으로 받는다. 이 떄 가중치행렬 $W_{v\times m}$ 을 각 단어에 곱하여 그 값을 projection layer에 전달한다. 이 때 주변단어는 희소벡터 형태로 표현된다.
 + **Projection layer**: 4개의 주변단어와 가중치행렬을 곱하여 4개의 output($1 \times m$ vector)을 산출한다. 이 때 각 output을 $v_{fat}, v_{dog}, v_{delicious}, v_{min}$이라 하자. Projection layer에서는 이 네 개의 벡터의 평균을 구하여 벡터 $v = \frac{v_{fat}+v_{dog}+v_{delicious}+ v_{min}}{2*\text{window}} $ 를 만든다.
 + **Output layer**: Projection layer에서 구한 벡터 $v$와 가중치행렬을 transpose한 $W^\prime$ 행렬을 곱하여 output layer의 input으로 전달한다. 이 값을 $z$ 라 하자. 그리고 최종적으로 output layer의 ouput은 $z$에 softmax함수를 적용하여 구할 수 있다. 
 + Output layer에서 구한 최종 output $\hat{y}$와 예측하고자 하는 대상인 **ate**의 희소벡터 $y$ 와의 cross entropy를 계산하여 loss를 구한다.
 
<br>
![pic](/assets/cbow.jpg)


### (2) Skip-Gram
SKip-Gram은 중심단어를 이용해 주변단어를 예측하는 방법으로, 앞서 다룬 CBOW 방법과 반대 과정이다.
<br>
![pic](/assets/skipgram.jpg)




---


**참고문헌**
<br>
<https://wikidocs.net/22660>
